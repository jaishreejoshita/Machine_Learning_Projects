{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "18aRCHLL0ZunersYrD55xvHVq16MDBs9V",
      "authorship_tag": "ABX9TyM1md6tLpWSk386KgrgapXi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaishreejoshita/Machine_Learning_Projects/blob/main/Data_Pre_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BWC-Ar0dIlCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process to do data pre-processing through the Machine Learning on salary structure of individuals from different countries"
      ],
      "metadata": {
        "id": "MmMQuXXBIr4S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lN7661sxIr19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Libraries"
      ],
      "metadata": {
        "id": "yalsozBNIl1G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ytZ6qY5Iey5I"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the dataset"
      ],
      "metadata": {
        "id": "OhcKMVnOIL7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Bg5Lz97QtB1",
        "outputId": "83346c69-aef3-49bf-b0a0-61d9d9cf0ac1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/Data_preprocessing.csv')\n",
        "print (df)\n",
        "X = df.iloc[:, :-1].values #to convert data into numpy array\n",
        "Y = df.iloc[:, -1].values"
      ],
      "metadata": {
        "id": "XmW6yl6aIPDw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef0a9c86-af3a-4b39-ee02-68e8a5c75435"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Country   Age   Salary Purchased\n",
            "0   France  44.0  72000.0        No\n",
            "1    Spain  27.0  48000.0       Yes\n",
            "2  Germany  30.0  54000.0        No\n",
            "3    Spain  38.0  61000.0        No\n",
            "4  Germany  40.0      NaN       Yes\n",
            "5   France  35.0  58000.0       Yes\n",
            "6    Spain   NaN  52000.0        No\n",
            "7   France  48.0  79000.0       Yes\n",
            "8  Germany  50.0  83000.0        No\n",
            "9   France  37.0  67000.0       Yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TH_zqXihQBM6",
        "outputId": "1e4398f9-85ab-4786-c2d9-225b342f0dc2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 nan]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' nan 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18I5WlzTQTsi",
        "outputId": "2a60da80-9696-4f72-a562-e2919fad3bf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Scikit-learn to handle missing values"
      ],
      "metadata": {
        "id": "q1vvjS6pRr28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
        "imputer.fit(X[:, 1:3]) #here 3 means column 2 as python starts indexing from 0\n",
        "X[:, 1:3] = imputer.transform(X[:,1:3])\n",
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bds32CRyR6q2",
        "outputId": "99e71a4a-63b7-4bfc-ac94-a3dfd0012073"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 63777.77777777778]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' 38.77777777777778 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding Categorical data:"
      ],
      "metadata": {
        "id": "WQtiv8IL4Nt9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding the independent variable"
      ],
      "metadata": {
        "id": "NNMnup-84Tw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder #OneHotEncoder to transfer the categorical data into binary digits\n",
        "ct = ColumnTransformer(transformers = [('encoder', OneHotEncoder(), [0])], remainder = 'passthrough')\n",
        "X = np.array(ct.fit_transform(X))\n",
        "print (X)"
      ],
      "metadata": {
        "id": "D8WEI3ZaSpj6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1317a15-1397-4473-c861-b0336e3c3e53"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [0.0 1.0 0.0 30.0 54000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 35.0 58000.0]\n",
            " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding the dependent variables\n",
        "Note: *We leverage LabelEncoder from Scikit learn's preprocessing library since it contains only two distinct categories compare to OneHotEncoding.\n",
        "\n",
        "simple and Effective way to encode binary categories variables, where the order the labels doesn't matter, unlike in ordinary encoding.*"
      ],
      "metadata": {
        "id": "nDKLwWXj-1gt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "Y = le.fit_transform(Y) #the transform data fit back to the numpy array #LabelEncoder\n",
        "print (Y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xfMwmfe5C-h",
        "outputId": "005975ba-c5da-4cf1-886a-b37b0f2598f3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 0 1 1 0 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting the data into training and testing set\n",
        "Scikit-learn offers a function called train_test_split within its model _selection module. This fucntion efficiently splits your data into training and testing sets, perfectly suited for model needs.\n",
        "\n",
        "It generates two pairs:\n",
        "\n",
        "*   A training set containing features (X_train) and target variables (Y-train)\n",
        "*   A testing set containing features (X_test) and target variables (Y-test)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LqWwT4FKBC6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#create four variables: X_train, X_test, Y_train, Y_test\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 1)\n"
      ],
      "metadata": {
        "id": "fCV3hbyPBNT7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SN20oL9CENLI",
        "outputId": "e37652d2-80ed-4166-e00f-d945b2cfa9a2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 35.0 58000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOh-3naxEZJo",
        "outputId": "6b65268a-b3a2-44f7-f0a4-fc0bd4b0184a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 1.0 0.0 30.0 54000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (Y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ODTY4Y-EtuT",
        "outputId": "2bdf98ff-a147-4a76-acbf-da5b26f69b46"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 0 1 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (Y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmuVjp_vEwnK",
        "outputId": "44a3aa59-ead1-4f3f-e901-ef2ae0d59f24"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature scaling -\n",
        "\n",
        "Even after splitting our data into training and testing sets, we might encounter issues if our features have significantly different ranges. This is where feature scaling comes in.\n",
        "\n",
        "Feature scaling is a data preprocessing technique that transforms our features to a common scale, that prevents one feature from dominating over others, ensuring that each feature contributes proportionately to the learning process of the machine learning model. If features have significantly different ranges and are not scaled, the model might give more weight to features with larger scales, leading to biased results. By scaling the features to a common scale, feature scaling helps mitigate this issue, ensuring that all features are equally considered during model training and evaluation.\n",
        "\n",
        "It's important to note that feature scaling is applied after the train-test split because the test set is supposed to be a brand new set on which we will evaluate our machine learning model. We're not supposed to work with it for training to avoid data leakage. This maintains the integrity of our evaluation process, allowing us to assess the performance of our model on unseen data accurately.\n",
        "\n",
        "There are two common techniques for feature scaling: Normalization and Standardization.\n",
        "\n",
        "\n",
        "*   Normalisation is a good choice when we don't know the underlying distribution of our data\n",
        "\n",
        "*   Standardization is good to use when our data follows a normal distribution\n",
        "\n",
        "**StandardScaler** class from the preprocessing module, which facilitates standardization on both the matrix of features of the training set and the matrix of features of the test set.\n",
        "\n"
      ],
      "metadata": {
        "id": "TwuKtkfpE8Bi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "#lets create an object and call it sc to perform standardization\n",
        "#note that we dont use scaling features for dummy variables\n",
        "sc = StandardScaler()\n",
        "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\n",
        "#fit and transform\n",
        "#lets apply the same transformation to testing data\n",
        "X_test[:, 3:] = sc.transform(X_test[:, 3:])\n",
        "#we are just using transform in the testing data as we already computed mean and standard deviation from the training data"
      ],
      "metadata": {
        "id": "GVbhA1TXFnnB"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKrWXgBdIPUP",
        "outputId": "b6cd51ba-3ae0-45b5-c447-649c679ef70d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 0.0 1.0 -0.1915918438457856 -1.0781259408412427]\n",
            " [0.0 1.0 0.0 -0.014117293757057902 -0.07013167641635401]\n",
            " [1.0 0.0 0.0 0.5667085065333239 0.6335624327104546]\n",
            " [0.0 0.0 1.0 -0.3045301939022488 -0.30786617274297895]\n",
            " [0.0 0.0 1.0 -1.901801144700799 -1.4204636155515822]\n",
            " [1.0 0.0 0.0 1.1475343068237056 1.2326533634535488]\n",
            " [0.0 1.0 0.0 1.4379472069688966 1.5749910381638883]\n",
            " [1.0 0.0 0.0 -0.7401495441200352 -0.5646194287757336]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sC48yCHSIUTo",
        "outputId": "469c8449-c2ae-4162-cdff-6770e11ffe63"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 1.0 0.0 -1.4661817944830127 -0.9069571034860731]\n",
            " [1.0 0.0 0.0 -0.44973664397484425 0.20564033932253029]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sA2TQxxcIXHV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}